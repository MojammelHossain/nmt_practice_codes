{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all needed jupyter notebooks and library\n",
    "from ipynb.fs.full.batch import *\n",
    "from ipynb.fs.full.vocab import *\n",
    "from ipynb.fs.full.encoder_decoder import *\n",
    "from torch import optim\n",
    "import pickle\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly taken some news articles\n",
    "en_sentence = [\"Bangladesh is yet to conduct an epidemiological surveillance which experts say is very important to understand the gravity of the coronavirus situation and will help take any policy decision in this regard\".lower(), \"They said such surveillance would have allowed health officials to suggest the government about the next course of action like when and how the ongoing shutdown should be relaxed or withdrawn which area is highly vulnerable and what measures to be taken\".lower()]\n",
    "bn_sentence = [\"এক প্রশ্নের জবাবে আইনমন্ত্রী বলেন অধ্যাদেশ অনুসারে প্রয়োজনবোধে যেকোনো সময় ডিজিটাল পদ্ধতিতে মামলার শুনানি ও নিষ্পত্তি করতে অডিও ও ভিডিও কনফারেন্সের মাধ্যমে আদালতের কার্যক্রম পরিচালনা করা যাবে\", \"চলমান করোনাভাইরাস প্রাদুর্ভাবের কারণে গত মার্চ থেকে আপিল বিভাগ ও হাইকোর্ট বিভাগসহ দেশের সব আদালতের নিয়মিত কার্যক্রম বন্ধ রয়েছে চলমান বন্ধের সময় ভার্চুয়াল আদালত পরিচালনার অনুমতি দেওয়ার জন্য একাধিক আইনজীবী প্রধান বিচারপতিকে অনুরোধ করেছিলেন প্রধান বিচারপতি সৈয়দ মাহমুদ হোসেন আপিল\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create LangVocab object and sort the dictionary\n",
    "en_vocab = LangVocab('en')\n",
    "bn_vocab = LangVocab('bn')\n",
    "for i in range(2):\n",
    "    en_vocab.sentence_To_word(en_sentence[i])\n",
    "    bn_vocab.sentence_To_word(bn_sentence[i])\n",
    "en_vocab.sort_dict()\n",
    "bn_vocab.sort_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize word dimentions to convert\n",
    "hidden_size = 300\n",
    "\n",
    "# initialize for holding pre-trained word embedding vetors\n",
    "weights_en = np.zeros((en_vocab.num_token, hidden_size))\n",
    "weights_bn = np.zeros((bn_vocab.num_token, hidden_size))\n",
    "\n",
    "# randomly (normal distribution) initialize first 4 token of vocabulary [_sos, _eos, _unk, _pad] \n",
    "for i in range(4):\n",
    "    a = np.random.normal(scale=0.6, size=(1, hidden_size))\n",
    "    weights_en[i] = a\n",
    "    weights_bn[i] = a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-trained embeddings vectors load from glove word embedding\n",
    "# english total token 400001\n",
    "# bengali total token 178153\n",
    "# load the pre-trained word embeddings vector into dictionary\n",
    "en_dict = {}\n",
    "bn_dict = {}\n",
    "with open('dict_embed_weights/en_dict.txt', 'rb') as f:\n",
    "    en_dict = pickle.load(f)\n",
    "with open('dict_embed_weights/bn/bn_dict.txt', 'rb') as f:\n",
    "    bn_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of token in original : 63\n",
      "Number of token found : 59\n",
      "Number of token in original : 63\n",
      "Number of token found : 58\n"
     ]
    }
   ],
   "source": [
    "# count the words number found in pre-trained word embeddings dictionary\n",
    "words_found_en = 0\n",
    "words_found_bn = 0\n",
    "for i in range(4, en_vocab.num_token):\n",
    "    \n",
    "    # choose the pre-trained embeddings vector of corresponding word for english\n",
    "    # if not found then initialize it with random values (normal distribution)\n",
    "    try: \n",
    "        weights_en[i] = en_dict[en_vocab.index_To_token[i]]\n",
    "        words_found_en += 1\n",
    "    except KeyError:\n",
    "        weights_en[i] = np.random.normal(scale=0.6, size=(1, hidden_size))\n",
    "for i in range(4, bn_vocab.num_token):\n",
    "    \n",
    "    # choose the pre-trained embeddings vector of corresponding word for bangla\n",
    "    # if not found then initialize it with random values (normal distribution)\n",
    "    try: \n",
    "        weights_bn[i] = bn_dict[bn_vocab.index_To_token[i]]\n",
    "        words_found_bn += 1\n",
    "    except KeyError:\n",
    "        weights_bn[i] = np.random.normal(scale=0.6, size=(1, hidden_size))\n",
    "\n",
    "print(\"Number of token in original : \" + str(en_vocab.num_token))\n",
    "print(\"Number of token found : \" + str(words_found_en))\n",
    "print(\"Number of token in original : \" + str(bn_vocab.num_token))\n",
    "print(\"Number of token found : \" + str(words_found_bn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create batch\n",
    "length = en_vocab.max_length if en_vocab.max_length > bn_vocab.max_length else bn_vocab.max_length\n",
    "batch_size = 1\n",
    "batch = Batch(batch_size = batch_size, max_length = length, seed = 1)\n",
    "batches = batch.get_batches(in_lang = en_vocab, tar_lang = bn_vocab, in_sentences = en_sentence, tar_sentences = bn_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate inputs and targets vector\n",
    "inputs = batches[0][0]\n",
    "targets = batches[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before initialize pre-trained embedding vector encoder : tensor([[ 1.4147, -0.0854,  0.9175,  0.0181,  0.3739, -0.1630, -0.8463, -0.6528,\n",
      "         -0.7771, -1.1314, -0.8318, -0.5424, -0.5617,  0.5578, -0.7622,  0.3717,\n",
      "         -1.3642,  0.1076, -1.5307, -1.0777, -0.6573,  1.7305,  0.8068, -0.4060,\n",
      "          0.3648,  1.2223, -1.8405, -0.5603, -1.0498, -2.3492, -0.7260, -0.8321,\n",
      "          1.5374,  1.1163,  0.9164,  0.0442,  0.2131, -0.6654, -0.5569,  0.9620,\n",
      "         -0.9159,  0.2355,  0.8549, -0.2426,  0.4443,  0.8974, -0.2509, -2.3011,\n",
      "          0.3318,  0.0437, -0.4704, -0.1118, -0.0834, -2.3521,  0.0058,  0.5663,\n",
      "          0.3209, -0.0513, -0.7264,  0.0960,  0.6562,  0.2912, -0.5171,  0.2734,\n",
      "          0.5427, -0.3862,  1.1704, -0.4868,  1.1555,  1.2038, -0.5833,  0.7980,\n",
      "          0.5164, -0.8668,  0.0736, -0.3150, -0.9301,  1.2166, -1.2124,  0.1488,\n",
      "          0.5701,  1.7268,  0.0054, -0.2173, -0.4522, -1.1454, -0.0724,  0.9688,\n",
      "         -1.0323, -0.1418,  0.4780,  1.2733,  1.7928,  0.5636,  0.5681,  0.5466,\n",
      "          0.2792, -0.3379,  0.0299,  1.0092, -0.3477,  1.0263, -1.4577, -1.7165,\n",
      "          2.1101,  0.7428, -0.6179, -0.3109,  0.2188, -0.0951, -0.3629,  1.0489,\n",
      "          1.2869, -0.1191,  0.7149, -0.6492, -0.7249,  0.4377, -0.4205, -0.7588,\n",
      "          0.2483, -1.0567, -0.1816,  0.0036, -1.0867, -0.4350,  0.0261,  0.3017,\n",
      "          1.1729, -1.8375, -0.2478,  2.0337, -0.2266,  1.1074, -0.3906, -0.2237,\n",
      "         -0.7752,  1.4636,  0.3407,  1.2703,  1.0453,  1.4611,  0.4247, -0.1359,\n",
      "          1.1550,  0.6434,  0.2760,  0.7284, -1.1591,  0.0941, -0.5053, -0.0854,\n",
      "          0.9602, -0.8784, -0.1735, -0.0572,  0.5518, -1.0070, -0.0545, -1.5440,\n",
      "         -0.2103,  0.5064, -0.8560, -0.4316, -0.3649, -0.7825, -1.4537, -1.5106,\n",
      "          1.3635, -0.0437,  0.5042, -2.0304,  0.3943, -0.3975, -0.1501, -2.1718,\n",
      "         -0.5738, -1.0293,  1.6693,  1.4403, -0.3516,  0.3840,  0.9286,  1.0351,\n",
      "         -0.8411, -0.4831,  0.7941, -0.6688, -1.6083, -0.2557, -0.6125,  0.6951,\n",
      "          0.1748, -0.9716, -0.1394,  0.2705,  0.9054, -1.0565, -0.1872,  1.1922,\n",
      "          1.8712, -1.9605,  1.6606, -1.4598,  0.4665, -0.7843,  0.2599,  0.2276,\n",
      "          1.5146,  0.2357,  1.5514,  0.3655, -1.1378,  0.4543, -0.2245,  0.7271,\n",
      "          1.3437,  0.1861, -0.7435, -2.6774,  0.6251,  0.5062, -2.7938,  0.5427,\n",
      "          0.8043,  2.4666, -0.2642, -0.2085,  0.2143,  1.5901,  0.8737, -1.9865,\n",
      "          0.1622,  0.1942, -1.3869, -1.7593,  0.2400, -0.4948, -0.5464,  0.1485,\n",
      "         -1.1994, -0.8853, -1.6113,  0.7927,  0.2771, -0.2459, -0.2772, -0.8574,\n",
      "          1.0543, -0.0730,  0.1141,  1.0488,  0.4773,  0.7136, -1.0911,  0.6585,\n",
      "         -0.1822, -0.2302, -1.1860, -0.3251, -0.0043, -0.3464, -0.4733,  2.7928,\n",
      "          0.3187, -0.5244,  1.8199,  1.1200,  0.5892, -0.5721,  0.6068,  0.3302,\n",
      "         -0.2680, -1.1435, -0.6685,  0.3014, -2.0003,  0.9424, -0.9190,  0.7284,\n",
      "          1.0418, -1.3294,  0.9028,  0.1621, -0.2456, -1.6522,  0.4757, -1.5758,\n",
      "         -1.0454,  1.9821,  1.6718, -0.5563, -0.7740, -1.2046,  0.3250,  1.1124,\n",
      "         -1.4577, -0.9975, -0.9284, -1.1321]], grad_fn=<EmbeddingBackward>)\n",
      "Before initialize pre-trained embedding vector decoder : tensor([[-0.1228, -2.0759, -1.5322, -0.4954, -1.6140,  0.7288,  1.2708, -1.9229,\n",
      "         -2.7744, -1.6484, -2.0822, -0.3910, -0.6460,  0.0413, -0.4468, -0.5557,\n",
      "          1.0756, -0.5218, -0.6270,  0.6832, -0.5795, -0.4470,  1.3258, -0.8069,\n",
      "         -0.2530, -0.0054, -1.1904,  0.1200,  0.5286,  2.2294,  1.1329,  0.2918,\n",
      "         -0.2019, -0.0654, -2.2790, -0.4088, -0.7371, -0.2381, -0.9162,  0.2426,\n",
      "         -0.9538, -1.0523,  0.1789, -0.7512,  0.7918, -1.6507, -0.1466,  0.2692,\n",
      "         -0.6484, -1.8070, -0.1142, -0.3637,  2.1360,  0.1450, -0.3515, -1.8963,\n",
      "         -1.1725,  0.0771, -1.0923, -0.8951, -0.7812, -1.0936, -0.6211, -1.0750,\n",
      "         -0.5908, -0.6986, -1.0781,  0.5000,  0.8197,  0.6689, -0.0321,  0.5525,\n",
      "         -1.7425, -1.0510, -1.0508, -0.1601, -1.6260,  0.5357, -0.6205, -0.3540,\n",
      "          0.3185,  0.0447,  0.6759, -0.3296,  0.5292, -0.1585, -0.5698,  0.3233,\n",
      "          0.0558, -2.3895,  3.1951, -0.6736, -0.3040,  1.3731, -0.0987, -0.3889,\n",
      "         -0.7637,  0.7692, -0.1370, -1.4970,  1.9802, -0.7533, -0.9244,  0.3141,\n",
      "          1.2832, -0.5534, -1.7244, -0.2424,  0.0499,  1.6690, -0.0887, -1.2077,\n",
      "         -0.0214,  0.5706, -0.5039, -0.8165,  0.0484,  0.0746, -0.1621, -0.3452,\n",
      "          1.6951, -0.8024, -0.0676, -1.0847, -1.5561, -0.8251, -0.5352,  0.1862,\n",
      "          0.4171,  0.9808, -1.4678,  0.5433, -1.2096,  1.1923,  0.2732, -0.7080,\n",
      "          1.6809,  0.7569, -0.0453, -1.2627,  0.7423,  0.2806, -0.5813,  0.9893,\n",
      "         -0.2758,  1.1237,  0.8968,  0.3131, -1.6235,  0.4276, -0.7477,  0.1714,\n",
      "          0.1860,  0.0554, -2.1941,  0.4065, -0.3415, -0.1925, -1.6439, -1.8632,\n",
      "          0.5863, -0.6494,  1.2563,  0.1086, -1.4053, -0.3971,  1.3712,  0.7440,\n",
      "         -0.4922, -0.3786,  0.2728,  0.7388, -0.5472, -1.5033, -0.6232,  0.6886,\n",
      "         -0.0202,  0.7925, -1.0411,  0.9007,  1.2880,  0.9036, -0.8496, -1.3861,\n",
      "         -0.5470, -0.6792,  0.9383,  0.5893, -0.0040, -0.7878, -0.8012, -1.7074,\n",
      "         -0.7991, -0.3827,  0.8927,  0.3816, -2.1682,  1.8427, -2.3434, -0.4053,\n",
      "         -1.2800,  0.9200, -0.4790, -0.1644,  0.8687, -0.4454, -0.6548, -0.4221,\n",
      "          0.1913, -1.2871, -1.7258, -2.2218, -0.3613,  1.4777,  0.7350,  0.1348,\n",
      "         -1.0683,  0.3848,  0.5898,  0.8761, -1.5084,  0.3562, -0.4053, -1.8524,\n",
      "          1.8229, -1.1520, -0.2282,  0.5953,  0.5287, -0.5468, -0.9222, -1.5343,\n",
      "          0.6183, -0.2508, -1.9186, -0.5174,  0.7920, -0.3382, -0.6779, -1.3986,\n",
      "         -0.3578,  1.3361, -0.3722, -0.4624,  1.0924,  0.2709, -0.1426,  1.5635,\n",
      "         -1.7639,  1.0510,  0.5913, -0.4609, -1.8852,  1.3718,  0.0165,  0.9170,\n",
      "         -0.5667, -0.2150, -1.0161, -0.3611, -0.1808,  0.5492,  0.7923,  1.3361,\n",
      "         -0.3544, -0.9034, -1.3555, -1.8979, -0.5731,  0.5619,  0.9054, -0.3017,\n",
      "         -0.1765, -0.6548, -0.8643,  0.6134,  0.7082, -0.8364,  1.7262, -1.5301,\n",
      "          0.9719, -0.5371, -0.6972, -2.7851,  1.0778, -0.8064,  0.0470, -0.4162,\n",
      "          0.0106,  0.0122, -0.0131, -1.0629, -1.2317, -0.3264, -1.0252,  0.7563,\n",
      "          0.2870,  0.5685,  1.0934,  0.6960]], grad_fn=<EmbeddingBackward>)\n",
      "After initialize pre-trained embedding vector encoder : tensor([[ 1.5623e-01,  3.9512e-02, -2.4926e-02, -5.6969e-01,  2.4714e-01,\n",
      "         -8.1321e-01,  9.5417e-01,  3.4287e-01, -3.1652e-01, -1.4253e+00,\n",
      "          5.5303e-01, -5.1177e-01, -1.2889e-01, -5.8871e-01,  2.0345e-01,\n",
      "         -1.7584e-01, -5.1390e-01,  1.2284e-02, -6.3512e-01, -4.7774e-01,\n",
      "         -1.4560e-02, -3.5894e-01,  1.1665e-01,  5.8085e-01, -5.7562e-01,\n",
      "          2.8406e-01,  1.4280e-01, -3.8228e-02,  4.9808e-02, -1.4718e-01,\n",
      "          8.9808e-02,  1.7262e-01,  6.3398e-02,  1.5434e-01, -3.7355e-01,\n",
      "         -2.4036e-01,  4.4372e-01, -1.3826e-01, -4.6264e-01, -4.9693e-02,\n",
      "          7.4430e-01, -3.5426e-02,  2.0100e-01,  5.4757e-01,  1.8120e-01,\n",
      "         -9.7867e-01, -5.0074e-01, -9.3119e-02, -2.3842e-01, -6.9147e-01,\n",
      "          1.1682e-02, -2.3328e-01,  2.7558e-01, -2.4080e-02,  6.2982e-01,\n",
      "          1.4431e-01,  2.6974e-01, -1.4964e-01, -5.6360e-02, -1.2848e-01,\n",
      "         -5.0717e-01,  2.0053e-01, -8.2646e-01, -8.7022e-01,  2.2535e-01,\n",
      "          3.4958e-01,  3.9725e-01,  2.0364e-01, -5.5978e-01,  3.1992e-01,\n",
      "          2.1454e-01,  3.9311e-01,  3.7578e-02, -5.5798e-01,  4.6613e-01,\n",
      "         -2.0389e-01,  3.9757e-01,  1.5018e-01, -2.0250e-02,  4.8402e-02,\n",
      "          9.1366e-01,  2.2434e-01,  5.3715e-02,  1.0315e-01, -4.0362e-01,\n",
      "          6.1974e-02,  3.0222e-02, -9.9266e-01,  2.3078e-01, -5.9314e-03,\n",
      "          2.9401e-01,  1.4082e-01,  3.2971e-01, -1.5506e-01,  2.1984e-01,\n",
      "          5.4065e-01,  3.8677e-01, -8.5856e-01,  6.1629e-03,  2.5033e-01,\n",
      "          4.0584e-01, -6.2771e-02, -2.5918e-02, -5.8348e-01,  3.9589e-01,\n",
      "         -1.2467e-01,  1.1329e+00,  2.9692e-01,  1.0511e-01,  1.6069e-01,\n",
      "          2.2573e-01, -1.1327e+00, -5.0875e-01,  3.5518e-01,  1.1510e+00,\n",
      "          5.0171e-02,  2.6714e-01,  6.0128e-01,  4.5453e-01, -5.1603e-01,\n",
      "         -2.7143e-01, -3.5794e-03, -4.8545e-02,  2.7600e-01,  1.1326e-01,\n",
      "          4.2573e-01, -2.7283e-01, -3.9335e-01,  7.8704e-01,  7.2588e-01,\n",
      "         -2.7016e-01,  2.6673e-01,  5.8095e-01, -1.8656e-01, -1.5010e-01,\n",
      "         -3.7581e-04,  7.6107e-01, -9.9241e-02, -3.6347e-01,  1.9647e-01,\n",
      "         -2.6769e-01,  9.1505e-01, -3.5245e-01, -5.4221e-01, -8.9978e-01,\n",
      "          1.0149e-01,  4.5101e-01, -1.1006e-01,  1.6674e-01,  2.0985e-01,\n",
      "          4.4515e-01, -8.4116e-01, -6.4558e-02,  2.2770e-01, -4.2904e-01,\n",
      "          6.3940e-01, -4.4503e-01,  2.0450e-02,  4.3738e-01, -1.3689e-01,\n",
      "         -5.4455e-01,  6.5698e-01, -3.3538e-01,  3.1022e-01, -3.5187e-02,\n",
      "         -1.6273e-01,  5.2276e-01, -3.2623e-01, -6.7950e-01,  3.5456e-01,\n",
      "         -1.6725e-01,  1.5051e-01, -3.1898e-02, -1.0988e-01,  7.3841e-02,\n",
      "          7.3786e-01, -2.7600e-01, -1.2010e-01, -7.7269e-01,  5.3855e-01,\n",
      "         -5.8286e-02, -3.5962e-01, -1.3548e-02,  1.2375e-01,  2.5391e-01,\n",
      "          5.8847e-01, -4.3459e-02,  6.7748e-01, -3.3016e-01, -3.9833e-01,\n",
      "          1.8760e-01,  5.2767e-01, -2.2181e-01,  2.1434e-01, -1.6671e-01,\n",
      "          8.6896e-01,  8.0614e-01, -4.6689e-01,  2.1132e-01, -5.8761e-01,\n",
      "          7.6414e-01,  2.1229e-01, -4.9032e-01, -3.0590e-01,  6.8939e-01,\n",
      "          1.2191e-01,  7.4567e-01,  6.5027e-02,  1.3619e-01, -4.0889e-01,\n",
      "         -3.1073e-01,  8.0734e-02,  2.1810e-01,  3.6492e-01, -1.9537e-01,\n",
      "         -4.1090e-01, -6.6245e-01, -5.5897e-01,  5.0445e-01,  3.1380e-01,\n",
      "          4.4665e-01,  1.6184e-01, -1.4244e-01, -4.5445e-01,  3.0982e-01,\n",
      "         -1.3695e-01,  3.0413e-01, -2.7380e-01, -3.0885e-01,  3.8312e-01,\n",
      "          9.5519e-02, -8.6049e-02, -6.2766e-01,  1.9595e-01,  5.9356e-01,\n",
      "         -4.3397e-02,  4.5242e-02, -1.6495e-01,  8.4019e-01, -5.7557e-01,\n",
      "          5.9771e-01,  2.8093e-01, -5.1656e-01,  1.6595e-01, -3.1616e-02,\n",
      "          2.3962e-01,  7.4332e-01,  2.1505e-01, -7.5373e-01,  1.0301e-01,\n",
      "          2.4470e-01, -3.7140e-01,  7.5053e-02, -2.0220e-01, -8.4593e-02,\n",
      "          2.6441e-01,  4.6060e-01,  1.5950e-01, -6.0431e-01,  3.9599e-01,\n",
      "          4.1752e-01, -3.9701e-01,  1.3560e-01,  4.7892e-02,  1.8001e-01,\n",
      "          1.8588e-01,  1.0737e-01, -3.4854e-01, -1.1459e-02,  1.1242e-01,\n",
      "         -4.9952e-01, -1.3278e-01,  4.0038e-02,  1.4877e-01,  2.4057e-01,\n",
      "          1.3550e-01, -8.7573e-01, -4.1480e-01,  1.9288e-01,  2.9861e-01,\n",
      "          2.4411e-01, -6.1081e-02,  5.4644e-01, -2.8108e-01,  4.8394e-01,\n",
      "          3.0322e-02,  2.2906e-02, -2.3011e-02, -1.6203e-01,  1.2645e-01,\n",
      "         -6.3028e-01, -2.7942e-01, -4.1192e-01,  2.1607e-01, -3.0893e-01,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          1.2915e+00,  1.2496e-01, -2.5026e-01, -8.3615e-02, -3.9094e-01]])\n",
      "After initialize pre-trained embedding vector decoder : tensor([[ 3.2509e-01, -8.6098e-01, -2.6855e-01, -4.1356e-01,  1.1385e-01,\n",
      "         -3.1579e-01, -5.6923e-01,  9.3286e-02,  7.3572e-01,  5.4303e-01,\n",
      "         -8.4033e-01, -5.2228e-01, -1.8866e-01, -5.1262e-01, -4.6871e-01,\n",
      "          4.7990e-01, -1.7242e-01,  3.9985e-01,  3.2247e-01,  3.3591e-01,\n",
      "          7.6855e-01, -3.4168e-01,  2.9821e-01, -1.4426e-01,  4.6129e-01,\n",
      "         -4.1432e-01,  4.9753e-01,  5.1368e-01, -2.3591e-01,  4.2208e-01,\n",
      "          1.2588e-01, -4.7551e-01,  8.5961e-01, -4.9544e-01, -2.1849e-01,\n",
      "         -8.0713e-02,  1.5788e-01, -3.3049e-02,  2.0216e-02,  2.5371e-01,\n",
      "         -2.4025e-01, -2.0854e-01,  3.5700e-01,  2.3532e-01,  2.8633e-02,\n",
      "         -4.2727e-01, -5.5547e-01,  2.5212e-01,  8.4813e-02,  5.9048e-01,\n",
      "          2.7492e-01,  2.9421e-01,  1.8115e-01,  3.2985e-01, -7.1027e-02,\n",
      "          5.8751e-01, -1.9190e-01,  3.4065e-02,  2.4787e-01,  3.6129e-01,\n",
      "          4.2183e-01, -8.4345e-02,  1.0779e-01,  5.8191e-01, -3.0436e-01,\n",
      "          2.8673e-02, -6.4300e-01,  1.6757e-02,  6.7035e-01, -6.3142e-01,\n",
      "         -2.2253e-01,  2.9962e-01, -1.0529e-01, -2.8185e-01,  1.2418e-01,\n",
      "          5.9238e-02, -4.5648e-01, -5.3052e-02, -1.1807e-01, -1.4267e-01,\n",
      "         -2.9632e-01, -5.2785e-01,  1.5245e-01,  2.4374e-01,  9.9508e-02,\n",
      "         -4.2192e-01,  3.3141e-02,  4.6790e-01,  3.4925e-01,  2.2862e-01,\n",
      "          3.0028e-01, -4.8664e-02, -3.1810e-01, -6.4693e-01, -4.9618e-01,\n",
      "         -7.7191e-02,  9.5231e-02,  8.0512e-01,  1.9973e-01,  1.0695e-01,\n",
      "          3.6796e-02,  7.8524e-01,  2.2136e-01,  4.1807e-01, -3.6677e-01,\n",
      "          1.4557e-01, -1.0491e-01,  7.5648e-02,  6.2121e-01,  2.6572e-01,\n",
      "          5.4806e-01, -4.4014e-01, -8.8746e-02,  5.7056e-01, -2.6948e-01,\n",
      "         -4.7311e-01,  3.6924e-01,  1.0403e-01,  1.5346e-01,  6.1542e-01,\n",
      "          3.7243e-01, -1.6216e-01, -2.3688e-01, -1.8278e-01, -5.7548e-01,\n",
      "          8.6539e-02, -1.0225e-01, -7.9652e-02, -8.0112e-01,  3.5917e-01,\n",
      "         -1.8115e-02,  9.9841e-01, -4.7202e-01,  1.3341e-01, -1.6927e-01,\n",
      "         -2.4539e-01,  5.4900e-03, -1.0421e+00, -2.0820e-01,  2.0089e-01,\n",
      "         -2.4432e-01,  2.7787e-02,  1.4070e-01, -5.5035e-01,  1.7091e-01,\n",
      "          3.1817e-01, -5.6370e-02,  3.0861e-01,  2.7502e-02,  2.0147e-01,\n",
      "         -5.3781e-01,  1.1057e-01, -1.7206e-01, -2.4022e-01, -4.0615e-01,\n",
      "         -2.9173e-01, -1.4912e-01, -1.9301e-01, -5.3654e-01, -3.5480e-01,\n",
      "          8.9497e-01,  1.3704e-01,  6.6157e-01, -5.6398e-01, -1.6367e-02,\n",
      "         -1.2661e+00,  6.1536e-02, -1.4905e-01,  1.3058e-01, -4.3067e-01,\n",
      "          1.5901e-01,  4.8877e-01,  2.0802e-01, -2.8038e-01,  8.1590e-03,\n",
      "          1.7603e-01, -4.1752e-01,  1.1411e-01,  2.0651e-01,  5.8834e-01,\n",
      "         -1.8400e-01,  4.1928e-01,  1.4665e-01,  1.1231e-01,  5.0851e-02,\n",
      "         -4.7510e-03, -5.3200e-01, -3.2305e-01, -3.6487e-01,  1.1525e-01,\n",
      "          6.3437e-01,  2.5801e-01,  2.6900e-01, -8.6777e-02,  2.0968e-01,\n",
      "          3.3101e-01, -3.6192e-01,  5.1077e-01,  1.8276e-01, -3.2481e-01,\n",
      "          1.3967e-01, -1.2709e-01, -1.5886e-01, -2.4450e-01,  3.2043e-02,\n",
      "         -3.8884e-02,  9.5813e-01,  4.3055e-01,  1.5600e-04,  1.0931e-01,\n",
      "          5.1357e-01,  2.9146e-01, -3.2185e-01, -4.1470e-01, -3.6232e-01,\n",
      "          5.4451e-01, -1.4974e-01,  3.2602e-02,  1.2067e-01,  4.2283e-01,\n",
      "          3.8753e-01, -7.5164e-01,  4.6968e-01,  1.0343e-02, -7.3457e-01,\n",
      "          2.1600e-02, -3.0125e-01, -4.0854e-01, -2.5128e-01, -3.6842e-01,\n",
      "         -8.6900e-02,  1.8990e-01,  6.3454e-01,  4.5061e-01,  4.3118e-01,\n",
      "          3.4490e-01,  9.9089e-02,  1.8584e-01, -2.1002e-01, -5.0052e-01,\n",
      "          3.8522e-01,  3.4427e-01, -4.4179e-01, -2.1358e-01,  2.9905e-01,\n",
      "         -2.3469e-02, -1.2685e-01,  2.5443e-01,  3.7509e-01,  3.3180e-01,\n",
      "          2.3954e-01, -4.7050e-03,  3.2354e-01,  8.4810e-02, -1.2481e-01,\n",
      "         -5.6089e-01,  5.7124e-01, -3.1781e-01,  1.6160e-01, -6.1136e-02,\n",
      "         -5.8573e-02,  3.1058e-01, -7.4360e-01, -5.6497e-01,  3.1474e-01,\n",
      "          1.6569e-02, -4.6451e-02, -3.2143e-01, -5.2671e-02, -2.9290e-01,\n",
      "         -1.3056e-01,  4.6667e-01,  2.0401e-01, -3.8103e-01,  2.8972e-02,\n",
      "         -1.2758e-01, -3.3170e-01, -3.9497e-01,  8.7712e-02, -6.5080e-02,\n",
      "         -9.9271e-02,  1.0685e+00,  1.1223e-01, -2.1485e-01, -3.6907e-01,\n",
      "         -4.0923e-01, -2.8170e-02,  3.0473e-01,  2.2755e-01, -9.6504e-02,\n",
      "          1.1447e+00, -4.6452e-01, -1.7751e-01,  1.5471e-02,  4.5165e-01,\n",
      "          1.4232e-01, -1.3589e-01,  3.2498e-01, -3.4155e-01,  3.4874e-02]])\n"
     ]
    }
   ],
   "source": [
    "# initialize encoder and decoder models\n",
    "encoder = LstmEncoder(in_vocab_size = en_vocab.num_token, word_dim=hidden_size, pad_idx = en_vocab.token_To_index['_pad'])\n",
    "decoder = LstmDecoder(tar_vocab_size = bn_vocab.num_token, word_dim = hidden_size, pad_idx = bn_vocab.token_To_index['_pad'])\n",
    "\n",
    "# learning rate\n",
    "lr = 0.001\n",
    "\n",
    "# initialize optimizers\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=lr)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=lr)\n",
    "\n",
    "# randomly print a embedding vector before initialize with pre-trained vectors\n",
    "print(\"Before initialize pre-trained embedding vector encoder : \" + str(encoder.word_embeds(torch.LongTensor([11]))))\n",
    "print(\"Before initialize pre-trained embedding vector decoder : \" + str(decoder.word_embeds(torch.LongTensor([12]))))\n",
    "\n",
    "# load the pre-trained embeddings vectors into models\n",
    "encoder.load_embed_weights(weights_en)\n",
    "decoder.load_embed_weights(weights_bn)\n",
    "\n",
    "# print above embedding vector after initialize with pre-trained vectors\n",
    "print(\"After initialize pre-trained embedding vector encoder : \" + str(encoder.word_embeds(torch.LongTensor([11]))))\n",
    "print(\"After initialize pre-trained embedding vector decoder : \" + str(decoder.word_embeds(torch.LongTensor([12]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before training encoder : tensor([[-1.9563e-01,  4.8127e-01, -2.4670e-01,  1.2751e-01,  3.0997e-01,\n",
      "         -5.7585e-02,  1.9660e-01,  2.4947e-01,  1.4405e-01, -2.1063e+00,\n",
      "          3.0094e-01, -1.1903e-01, -2.7087e-01,  2.6418e-01, -2.8135e-02,\n",
      "         -9.6218e-03, -6.3701e-02, -3.6602e-02, -3.9539e-01, -1.9931e-01,\n",
      "         -1.3172e-01,  4.1708e-01,  3.8747e-01, -2.4867e-01, -4.0963e-01,\n",
      "          3.0117e-01, -6.6700e-02, -4.0928e-01, -1.8198e-01,  4.1716e-01,\n",
      "          5.0072e-01,  4.1207e-01, -2.6712e-01, -1.8796e-01, -8.7328e-01,\n",
      "         -4.6190e-01,  1.1255e-01,  1.9181e-01,  4.0320e-01, -1.4160e-01,\n",
      "         -1.8509e-01, -2.7491e-01, -2.1240e-01,  1.8191e-01, -1.3362e-01,\n",
      "          2.5592e-01, -1.6499e-01,  9.0990e-02, -1.2049e-01,  3.3701e-01,\n",
      "          1.2667e-01, -5.2719e-01,  1.3359e-01,  1.6317e-01, -1.3175e-02,\n",
      "          3.6707e-01,  3.0000e-01,  3.5971e-01,  1.9285e-01,  9.2912e-03,\n",
      "          9.4887e-02,  3.2849e-01,  1.5767e-01,  2.4928e-01, -1.3388e-01,\n",
      "         -4.9632e-01,  2.4563e-01,  4.8527e-01, -3.5340e-01, -1.6990e-01,\n",
      "          3.6280e-01, -1.0072e-01,  2.1079e-01,  2.8626e-01,  1.7234e-01,\n",
      "          2.2902e-01,  6.3305e-02,  2.6053e-02, -3.3690e-01, -4.5906e-02,\n",
      "         -2.6874e-01, -2.8072e-04,  3.6887e-01, -6.9954e-02,  3.1475e-01,\n",
      "          3.0141e-01, -1.1042e-01,  1.2635e-01,  1.4873e-01,  1.9555e-01,\n",
      "         -4.1103e-01,  5.3414e-01, -3.4231e-01,  3.1414e-01,  2.6915e-02,\n",
      "          4.6154e-01, -2.0820e-01,  1.7026e-01,  2.9070e-01, -9.3704e-02,\n",
      "          1.6006e-01, -1.1457e-01, -2.4540e-02, -7.0862e-02, -3.0311e-01,\n",
      "          3.2727e-01,  2.1661e-01,  3.0989e-01, -3.0550e-01,  2.2702e-01,\n",
      "         -2.1938e-01, -5.4666e-01,  7.3684e-02, -2.0416e-01, -2.8822e-02,\n",
      "          3.6873e-01, -5.3940e-01, -6.0777e-02, -8.2640e-02, -3.1643e-01,\n",
      "          2.1133e-01, -6.3390e-01,  9.7148e-02,  4.1459e-01, -2.7142e-02,\n",
      "          4.9887e-01, -4.9129e-01,  1.4702e-01,  8.2600e-02,  8.7281e-02,\n",
      "         -2.4102e-01, -2.0840e-01, -6.9932e-02,  4.8917e-01, -6.0501e-02,\n",
      "         -7.6050e-03, -7.6334e-02, -1.6650e-01, -9.7181e-02,  1.1281e-01,\n",
      "         -4.2490e-01,  2.5519e-01,  1.2230e-01,  3.8433e-01, -2.2136e-01,\n",
      "          9.2306e-02,  1.1713e-01, -2.0399e-01,  2.0896e-02,  4.2158e-02,\n",
      "         -1.2169e-01, -1.8898e-01,  1.0419e-01,  2.0200e-01,  5.9587e-02,\n",
      "          1.9843e-01, -3.9445e-01, -2.9568e-01,  2.7117e-01, -6.3178e-02,\n",
      "          2.2316e-01, -3.7033e-01, -1.8546e-01,  4.9780e-01, -2.1548e-02,\n",
      "          2.8304e-01,  6.2456e-02, -2.1716e-01,  1.3276e-01, -2.1435e-01,\n",
      "          4.8942e-02, -3.9413e-02, -7.0321e-01, -1.6038e-02,  9.3201e-02,\n",
      "         -7.1235e-02, -2.9680e-01,  5.0649e-01,  2.4341e-01,  6.3492e-01,\n",
      "         -1.9164e-01,  1.3902e-01,  1.9112e-01,  5.6269e-02, -2.6604e-01,\n",
      "         -1.6213e-01, -2.9800e-01,  4.3035e-01,  5.8478e-01, -1.3301e-01,\n",
      "         -2.0404e-01, -3.9647e-01,  2.5292e-01, -1.3716e-01,  3.7798e-01,\n",
      "         -2.8922e-01,  2.1410e-01,  4.3657e-01,  8.1394e-02, -5.3666e-02,\n",
      "          6.6092e-01, -1.9177e-01,  1.5807e-01,  1.6073e-01,  2.0653e-01,\n",
      "         -1.8259e-01, -4.0779e-02,  4.5991e-01, -4.2338e-02,  1.0668e-01,\n",
      "          1.7142e-01,  3.0112e-01,  2.3260e-01,  4.9882e-02,  3.2827e-01,\n",
      "         -2.3358e-01,  1.9321e-01,  3.7349e-01,  7.1639e-02, -1.0584e-01,\n",
      "          5.5200e-01, -1.0953e-01,  5.0427e-02, -3.9834e-01,  2.2330e-01,\n",
      "          2.3159e-02,  1.6131e-01, -8.8837e-02, -1.5082e-01, -2.5822e-01,\n",
      "         -3.0015e-01,  1.2637e-02, -1.5221e-01, -3.3081e-01, -1.5796e-01,\n",
      "          4.2251e-01,  2.8959e-01,  1.3603e-01, -4.4917e-01, -2.7338e-01,\n",
      "          1.1770e-01,  1.4565e-01,  5.6783e-01,  1.0231e-01, -9.3597e-01,\n",
      "         -1.4156e-01,  2.6889e-01,  2.3354e-01,  1.8078e-01,  4.1739e-01,\n",
      "          2.0107e-02, -4.7957e-01, -1.8624e-02,  2.5683e-01,  4.9726e-01,\n",
      "         -7.7655e-03,  1.3772e-01, -1.3465e-01, -1.7735e-01,  2.0448e-01,\n",
      "         -1.7611e-01,  1.9226e-01,  1.1950e-01,  1.9199e-01, -3.0800e-01,\n",
      "          2.8276e-01, -3.7843e-01, -2.9576e-01,  1.0552e-01,  1.5098e-01,\n",
      "          1.3438e-01,  3.7831e-01,  1.2645e-01, -1.7134e-01,  1.7703e-01,\n",
      "          2.0824e-01, -2.7299e+00, -2.4950e-01,  3.3149e-01,  1.3897e-01,\n",
      "         -3.3547e-01,  1.7495e-01,  1.2598e-01,  2.1795e-01, -2.7668e-01,\n",
      "         -3.8286e-02,  1.1087e-01,  8.6324e-03,  1.5148e-01, -1.3083e-01,\n",
      "         -1.1784e-01, -4.7226e-01,  2.0011e-01,  3.8680e-02,  9.6292e-03,\n",
      "          1.7821e-01, -2.0447e-02, -8.7195e-01,  6.1883e-02, -1.7180e-01]])\n",
      "Before training decoder : tensor([[ 2.5467e-01, -7.2004e-01,  8.6804e-01, -1.8680e-01, -2.2749e-01,\n",
      "         -4.3246e-01, -1.0012e-01, -3.2527e-01, -4.6018e-02, -2.6165e-01,\n",
      "         -1.1244e+00, -2.1352e-01, -7.8877e-01, -7.6105e-01, -7.1015e-01,\n",
      "         -3.7338e-01,  4.7787e-01,  6.8339e-01,  5.3940e-01,  1.4671e-01,\n",
      "         -2.5929e-01,  1.0766e+00,  1.1965e-01,  2.3085e-01,  2.6334e-01,\n",
      "         -1.9730e-02,  5.7163e-01,  5.5877e-01, -3.9436e-02,  2.4501e-01,\n",
      "         -3.8074e-01, -1.5822e-01,  6.9710e-03, -1.3434e-01, -2.8466e-01,\n",
      "         -2.1146e-01, -4.5824e-01,  2.2336e-01, -4.0733e-02, -9.9456e-02,\n",
      "          7.0846e-01, -2.4373e-01, -2.9830e-01, -8.9379e-02, -8.1230e-02,\n",
      "          8.2390e-01,  1.5593e-01,  5.4764e-01, -2.4462e-01, -6.6450e-02,\n",
      "          7.8244e-01, -2.7421e-01,  1.2103e-01,  5.7933e-01, -7.9919e-01,\n",
      "          3.7049e-01,  3.4325e-01,  2.3648e-01,  6.1710e-03, -3.4833e-01,\n",
      "          3.8827e-01,  3.3827e-01,  5.4555e-01,  4.9514e-01, -3.1538e-01,\n",
      "          1.4080e-03, -7.0297e-01,  5.4724e-01,  4.9303e-01, -7.7210e-01,\n",
      "         -2.7549e-01,  2.1373e-01,  4.4914e-01,  4.0949e-01,  3.3911e-01,\n",
      "         -7.6654e-02,  9.4869e-02,  5.4273e-01, -7.0700e-03, -7.8511e-02,\n",
      "         -3.6048e-01, -6.2816e-01, -4.3217e-01,  8.3732e-02, -3.6635e-01,\n",
      "         -7.2707e-01,  2.4419e-01,  3.3355e-01,  5.7167e-01, -8.3797e-01,\n",
      "          3.3614e-01, -4.2606e-01,  6.1630e-01,  9.9138e-02, -5.2170e-01,\n",
      "         -2.5481e-01, -4.1534e-01,  6.7291e-01,  1.1994e-01,  4.2572e-01,\n",
      "         -6.6676e-02,  6.5460e-01,  5.3648e-01, -7.3492e-01,  2.5963e-01,\n",
      "          2.0669e-01,  2.3507e-01, -9.7276e-01,  6.4339e-02, -1.9077e-01,\n",
      "          1.5319e-01, -3.0744e-01, -5.1154e-01, -3.7655e-02,  6.8500e-01,\n",
      "         -4.6461e-01,  8.0113e-01, -2.9183e-02, -5.0648e-01,  3.8586e-01,\n",
      "         -6.5398e-02,  2.7993e-01, -8.3717e-01, -2.1683e-01,  9.1941e-02,\n",
      "         -3.8112e-01,  1.7509e-02, -2.6909e-01, -4.8255e-01,  3.4563e-01,\n",
      "         -8.4145e-02, -5.4615e-01, -2.2133e-01, -3.9467e-02,  7.8986e-02,\n",
      "          4.7085e-01, -3.7452e-01, -6.9906e-01,  2.1031e-01,  7.6282e-01,\n",
      "         -4.2968e-01,  2.8037e-01, -1.3978e-01,  9.1577e-02, -2.1171e-01,\n",
      "          5.1974e-01, -1.0706e-01, -6.5908e-01, -1.9817e-02,  6.1704e-02,\n",
      "         -2.6920e-01, -4.2324e-01, -2.5727e-01, -1.1332e+00,  5.4842e-01,\n",
      "         -1.9276e-01,  1.3888e-01,  6.5600e-04, -2.6425e-01,  2.8788e-01,\n",
      "         -3.8303e-01, -2.2063e-01,  3.2250e-01, -8.5665e-01,  1.3219e-01,\n",
      "         -6.6276e-01,  2.5129e-01,  3.6160e-01,  1.7219e-01, -5.7445e-01,\n",
      "          5.4912e-01,  1.1047e-01, -1.2598e-01, -4.8122e-01, -3.9152e-02,\n",
      "          1.0741e-02, -9.3878e-01, -2.7407e-01,  3.3202e-01,  7.4812e-01,\n",
      "          2.6133e-01,  4.3890e-02, -5.6942e-01,  9.0338e-01, -5.3741e-02,\n",
      "         -2.2510e-01, -3.5921e-01, -4.9862e-01,  9.7215e-01,  5.9707e-01,\n",
      "          2.6245e-01,  3.7020e-01,  4.0304e-01,  3.1944e-01,  4.0779e-01,\n",
      "         -8.4980e-03, -1.9092e-01, -2.1572e-01,  7.1311e-01, -7.1284e-01,\n",
      "         -2.5471e-01, -2.9023e-01, -3.9076e-02,  1.6413e-01,  7.1825e-01,\n",
      "          4.8772e-02,  4.7681e-01,  1.6674e-01, -1.2792e-01, -6.8149e-01,\n",
      "          3.7585e-01, -2.7078e-01,  2.5710e-01,  3.0042e-01,  5.4597e-01,\n",
      "          5.3950e-01,  1.4468e-01,  1.0725e-01,  1.0692e+00,  5.9641e-01,\n",
      "          3.4993e-01, -5.7120e-01,  9.6770e-03,  6.9843e-02,  4.5594e-02,\n",
      "          2.4593e-01,  2.7466e-01, -5.1913e-01,  4.0723e-01, -9.8846e-01,\n",
      "          4.4286e-01, -4.1862e-02, -6.9877e-02,  5.4232e-01,  1.4044e-01,\n",
      "          3.9721e-01, -4.0049e-01, -1.7743e-01,  2.0187e-01,  1.5657e-01,\n",
      "          1.1606e-01, -4.1098e-01,  1.8779e-01,  6.8846e-01,  9.9707e-02,\n",
      "          5.5745e-02, -3.0921e-01,  4.5167e-01, -4.3684e-02, -1.4969e-01,\n",
      "         -2.0072e-01,  8.7457e-01, -4.9820e-01,  2.6774e-01,  2.8823e-01,\n",
      "         -2.6198e-01,  6.1552e-01,  3.2395e-01, -4.1093e-01, -4.5236e-02,\n",
      "         -3.8060e-01, -1.5788e-01, -6.4926e-01, -5.8643e-01,  1.2712e-01,\n",
      "         -2.4896e-02, -2.9709e-01,  2.1081e-01,  1.9982e-01,  1.0430e-01,\n",
      "         -1.1570e-01, -5.3400e-01,  4.5455e-01, -5.5897e-02,  1.5313e-02,\n",
      "          5.2806e-02, -3.3330e-01, -5.6856e-02, -4.9061e-02,  9.3875e-01,\n",
      "          6.5269e-01,  8.1221e-01,  3.7162e-01, -3.1690e-01,  2.9151e-01,\n",
      "          5.8462e-02, -6.1372e-01, -2.3874e-01,  3.6812e-01, -3.4587e-02,\n",
      "          7.0075e-01, -2.9594e-01,  3.2229e-01,  2.8400e-03,  2.3719e-01,\n",
      "          8.1043e-01,  6.9595e-01,  3.4920e-01, -3.7094e-01, -4.4487e-02]])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After training encoder : tensor([[-1.9563e-01,  4.8127e-01, -2.4670e-01,  1.2751e-01,  3.0997e-01,\n",
      "         -5.7585e-02,  1.9660e-01,  2.4947e-01,  1.4405e-01, -2.1063e+00,\n",
      "          3.0094e-01, -1.1903e-01, -2.7087e-01,  2.6418e-01, -2.8135e-02,\n",
      "         -9.6218e-03, -6.3701e-02, -3.6602e-02, -3.9539e-01, -1.9931e-01,\n",
      "         -1.3172e-01,  4.1708e-01,  3.8747e-01, -2.4867e-01, -4.0963e-01,\n",
      "          3.0117e-01, -6.6700e-02, -4.0928e-01, -1.8198e-01,  4.1716e-01,\n",
      "          5.0072e-01,  4.1207e-01, -2.6712e-01, -1.8796e-01, -8.7328e-01,\n",
      "         -4.6190e-01,  1.1255e-01,  1.9181e-01,  4.0320e-01, -1.4160e-01,\n",
      "         -1.8509e-01, -2.7491e-01, -2.1240e-01,  1.8191e-01, -1.3362e-01,\n",
      "          2.5592e-01, -1.6499e-01,  9.0990e-02, -1.2049e-01,  3.3701e-01,\n",
      "          1.2667e-01, -5.2719e-01,  1.3359e-01,  1.6317e-01, -1.3175e-02,\n",
      "          3.6707e-01,  3.0000e-01,  3.5971e-01,  1.9285e-01,  9.2912e-03,\n",
      "          9.4887e-02,  3.2849e-01,  1.5767e-01,  2.4928e-01, -1.3388e-01,\n",
      "         -4.9632e-01,  2.4563e-01,  4.8527e-01, -3.5340e-01, -1.6990e-01,\n",
      "          3.6280e-01, -1.0072e-01,  2.1079e-01,  2.8626e-01,  1.7234e-01,\n",
      "          2.2902e-01,  6.3305e-02,  2.6053e-02, -3.3690e-01, -4.5906e-02,\n",
      "         -2.6874e-01, -2.8072e-04,  3.6887e-01, -6.9954e-02,  3.1475e-01,\n",
      "          3.0141e-01, -1.1042e-01,  1.2635e-01,  1.4873e-01,  1.9555e-01,\n",
      "         -4.1103e-01,  5.3414e-01, -3.4231e-01,  3.1414e-01,  2.6915e-02,\n",
      "          4.6154e-01, -2.0820e-01,  1.7026e-01,  2.9070e-01, -9.3704e-02,\n",
      "          1.6006e-01, -1.1457e-01, -2.4540e-02, -7.0862e-02, -3.0311e-01,\n",
      "          3.2727e-01,  2.1661e-01,  3.0989e-01, -3.0550e-01,  2.2702e-01,\n",
      "         -2.1938e-01, -5.4666e-01,  7.3684e-02, -2.0416e-01, -2.8822e-02,\n",
      "          3.6873e-01, -5.3940e-01, -6.0777e-02, -8.2640e-02, -3.1643e-01,\n",
      "          2.1133e-01, -6.3390e-01,  9.7148e-02,  4.1459e-01, -2.7142e-02,\n",
      "          4.9887e-01, -4.9129e-01,  1.4702e-01,  8.2600e-02,  8.7281e-02,\n",
      "         -2.4102e-01, -2.0840e-01, -6.9932e-02,  4.8917e-01, -6.0501e-02,\n",
      "         -7.6050e-03, -7.6334e-02, -1.6650e-01, -9.7181e-02,  1.1281e-01,\n",
      "         -4.2490e-01,  2.5519e-01,  1.2230e-01,  3.8433e-01, -2.2136e-01,\n",
      "          9.2306e-02,  1.1713e-01, -2.0399e-01,  2.0896e-02,  4.2158e-02,\n",
      "         -1.2169e-01, -1.8898e-01,  1.0419e-01,  2.0200e-01,  5.9587e-02,\n",
      "          1.9843e-01, -3.9445e-01, -2.9568e-01,  2.7117e-01, -6.3178e-02,\n",
      "          2.2316e-01, -3.7033e-01, -1.8546e-01,  4.9780e-01, -2.1548e-02,\n",
      "          2.8304e-01,  6.2456e-02, -2.1716e-01,  1.3276e-01, -2.1435e-01,\n",
      "          4.8942e-02, -3.9413e-02, -7.0321e-01, -1.6038e-02,  9.3201e-02,\n",
      "         -7.1235e-02, -2.9680e-01,  5.0649e-01,  2.4341e-01,  6.3492e-01,\n",
      "         -1.9164e-01,  1.3902e-01,  1.9112e-01,  5.6269e-02, -2.6604e-01,\n",
      "         -1.6213e-01, -2.9800e-01,  4.3035e-01,  5.8478e-01, -1.3301e-01,\n",
      "         -2.0404e-01, -3.9647e-01,  2.5292e-01, -1.3716e-01,  3.7798e-01,\n",
      "         -2.8922e-01,  2.1410e-01,  4.3657e-01,  8.1394e-02, -5.3666e-02,\n",
      "          6.6092e-01, -1.9177e-01,  1.5807e-01,  1.6073e-01,  2.0653e-01,\n",
      "         -1.8259e-01, -4.0779e-02,  4.5991e-01, -4.2338e-02,  1.0668e-01,\n",
      "          1.7142e-01,  3.0112e-01,  2.3260e-01,  4.9882e-02,  3.2827e-01,\n",
      "         -2.3358e-01,  1.9321e-01,  3.7349e-01,  7.1639e-02, -1.0584e-01,\n",
      "          5.5200e-01, -1.0953e-01,  5.0427e-02, -3.9834e-01,  2.2330e-01,\n",
      "          2.3159e-02,  1.6131e-01, -8.8837e-02, -1.5082e-01, -2.5822e-01,\n",
      "         -3.0015e-01,  1.2637e-02, -1.5221e-01, -3.3081e-01, -1.5796e-01,\n",
      "          4.2251e-01,  2.8959e-01,  1.3603e-01, -4.4917e-01, -2.7338e-01,\n",
      "          1.1770e-01,  1.4565e-01,  5.6783e-01,  1.0231e-01, -9.3597e-01,\n",
      "         -1.4156e-01,  2.6889e-01,  2.3354e-01,  1.8078e-01,  4.1739e-01,\n",
      "          2.0107e-02, -4.7957e-01, -1.8624e-02,  2.5683e-01,  4.9726e-01,\n",
      "         -7.7655e-03,  1.3772e-01, -1.3465e-01, -1.7735e-01,  2.0448e-01,\n",
      "         -1.7611e-01,  1.9226e-01,  1.1950e-01,  1.9199e-01, -3.0800e-01,\n",
      "          2.8276e-01, -3.7843e-01, -2.9576e-01,  1.0552e-01,  1.5098e-01,\n",
      "          1.3438e-01,  3.7831e-01,  1.2645e-01, -1.7134e-01,  1.7703e-01,\n",
      "          2.0824e-01, -2.7299e+00, -2.4950e-01,  3.3149e-01,  1.3897e-01,\n",
      "         -3.3547e-01,  1.7495e-01,  1.2598e-01,  2.1795e-01, -2.7668e-01,\n",
      "         -3.8286e-02,  1.1087e-01,  8.6324e-03,  1.5148e-01, -1.3083e-01,\n",
      "         -1.1784e-01, -4.7226e-01,  2.0011e-01,  3.8680e-02,  9.6292e-03,\n",
      "          1.7821e-01, -2.0447e-02, -8.7195e-01,  6.1883e-02, -1.7180e-01]])\n",
      "After training decoder : tensor([[ 2.5467e-01, -7.2004e-01,  8.6804e-01, -1.8680e-01, -2.2749e-01,\n",
      "         -4.3246e-01, -1.0012e-01, -3.2527e-01, -4.6018e-02, -2.6165e-01,\n",
      "         -1.1244e+00, -2.1352e-01, -7.8877e-01, -7.6105e-01, -7.1015e-01,\n",
      "         -3.7338e-01,  4.7787e-01,  6.8339e-01,  5.3940e-01,  1.4671e-01,\n",
      "         -2.5929e-01,  1.0766e+00,  1.1965e-01,  2.3085e-01,  2.6334e-01,\n",
      "         -1.9730e-02,  5.7163e-01,  5.5877e-01, -3.9436e-02,  2.4501e-01,\n",
      "         -3.8074e-01, -1.5822e-01,  6.9710e-03, -1.3434e-01, -2.8466e-01,\n",
      "         -2.1146e-01, -4.5824e-01,  2.2336e-01, -4.0733e-02, -9.9456e-02,\n",
      "          7.0846e-01, -2.4373e-01, -2.9830e-01, -8.9379e-02, -8.1230e-02,\n",
      "          8.2390e-01,  1.5593e-01,  5.4764e-01, -2.4462e-01, -6.6450e-02,\n",
      "          7.8244e-01, -2.7421e-01,  1.2103e-01,  5.7933e-01, -7.9919e-01,\n",
      "          3.7049e-01,  3.4325e-01,  2.3648e-01,  6.1710e-03, -3.4833e-01,\n",
      "          3.8827e-01,  3.3827e-01,  5.4555e-01,  4.9514e-01, -3.1538e-01,\n",
      "          1.4080e-03, -7.0297e-01,  5.4724e-01,  4.9303e-01, -7.7210e-01,\n",
      "         -2.7549e-01,  2.1373e-01,  4.4914e-01,  4.0949e-01,  3.3911e-01,\n",
      "         -7.6654e-02,  9.4869e-02,  5.4273e-01, -7.0700e-03, -7.8511e-02,\n",
      "         -3.6048e-01, -6.2816e-01, -4.3217e-01,  8.3732e-02, -3.6635e-01,\n",
      "         -7.2707e-01,  2.4419e-01,  3.3355e-01,  5.7167e-01, -8.3797e-01,\n",
      "          3.3614e-01, -4.2606e-01,  6.1630e-01,  9.9138e-02, -5.2170e-01,\n",
      "         -2.5481e-01, -4.1534e-01,  6.7291e-01,  1.1994e-01,  4.2572e-01,\n",
      "         -6.6676e-02,  6.5460e-01,  5.3648e-01, -7.3492e-01,  2.5963e-01,\n",
      "          2.0669e-01,  2.3507e-01, -9.7276e-01,  6.4339e-02, -1.9077e-01,\n",
      "          1.5319e-01, -3.0744e-01, -5.1154e-01, -3.7655e-02,  6.8500e-01,\n",
      "         -4.6461e-01,  8.0113e-01, -2.9183e-02, -5.0648e-01,  3.8586e-01,\n",
      "         -6.5398e-02,  2.7993e-01, -8.3717e-01, -2.1683e-01,  9.1941e-02,\n",
      "         -3.8112e-01,  1.7509e-02, -2.6909e-01, -4.8255e-01,  3.4563e-01,\n",
      "         -8.4145e-02, -5.4615e-01, -2.2133e-01, -3.9467e-02,  7.8986e-02,\n",
      "          4.7085e-01, -3.7452e-01, -6.9906e-01,  2.1031e-01,  7.6282e-01,\n",
      "         -4.2968e-01,  2.8037e-01, -1.3978e-01,  9.1577e-02, -2.1171e-01,\n",
      "          5.1974e-01, -1.0706e-01, -6.5908e-01, -1.9817e-02,  6.1704e-02,\n",
      "         -2.6920e-01, -4.2324e-01, -2.5727e-01, -1.1332e+00,  5.4842e-01,\n",
      "         -1.9276e-01,  1.3888e-01,  6.5600e-04, -2.6425e-01,  2.8788e-01,\n",
      "         -3.8303e-01, -2.2063e-01,  3.2250e-01, -8.5665e-01,  1.3219e-01,\n",
      "         -6.6276e-01,  2.5129e-01,  3.6160e-01,  1.7219e-01, -5.7445e-01,\n",
      "          5.4912e-01,  1.1047e-01, -1.2598e-01, -4.8122e-01, -3.9152e-02,\n",
      "          1.0741e-02, -9.3878e-01, -2.7407e-01,  3.3202e-01,  7.4812e-01,\n",
      "          2.6133e-01,  4.3890e-02, -5.6942e-01,  9.0338e-01, -5.3741e-02,\n",
      "         -2.2510e-01, -3.5921e-01, -4.9862e-01,  9.7215e-01,  5.9707e-01,\n",
      "          2.6245e-01,  3.7020e-01,  4.0304e-01,  3.1944e-01,  4.0779e-01,\n",
      "         -8.4980e-03, -1.9092e-01, -2.1572e-01,  7.1311e-01, -7.1284e-01,\n",
      "         -2.5471e-01, -2.9023e-01, -3.9076e-02,  1.6413e-01,  7.1825e-01,\n",
      "          4.8772e-02,  4.7681e-01,  1.6674e-01, -1.2792e-01, -6.8149e-01,\n",
      "          3.7585e-01, -2.7078e-01,  2.5710e-01,  3.0042e-01,  5.4597e-01,\n",
      "          5.3950e-01,  1.4468e-01,  1.0725e-01,  1.0692e+00,  5.9641e-01,\n",
      "          3.4993e-01, -5.7120e-01,  9.6770e-03,  6.9843e-02,  4.5594e-02,\n",
      "          2.4593e-01,  2.7466e-01, -5.1913e-01,  4.0723e-01, -9.8846e-01,\n",
      "          4.4286e-01, -4.1862e-02, -6.9877e-02,  5.4232e-01,  1.4044e-01,\n",
      "          3.9721e-01, -4.0049e-01, -1.7743e-01,  2.0187e-01,  1.5657e-01,\n",
      "          1.1606e-01, -4.1098e-01,  1.8779e-01,  6.8846e-01,  9.9707e-02,\n",
      "          5.5745e-02, -3.0921e-01,  4.5167e-01, -4.3684e-02, -1.4969e-01,\n",
      "         -2.0072e-01,  8.7457e-01, -4.9820e-01,  2.6774e-01,  2.8823e-01,\n",
      "         -2.6198e-01,  6.1552e-01,  3.2395e-01, -4.1093e-01, -4.5236e-02,\n",
      "         -3.8060e-01, -1.5788e-01, -6.4926e-01, -5.8643e-01,  1.2712e-01,\n",
      "         -2.4896e-02, -2.9709e-01,  2.1081e-01,  1.9982e-01,  1.0430e-01,\n",
      "         -1.1570e-01, -5.3400e-01,  4.5455e-01, -5.5897e-02,  1.5313e-02,\n",
      "          5.2806e-02, -3.3330e-01, -5.6856e-02, -4.9061e-02,  9.3875e-01,\n",
      "          6.5269e-01,  8.1221e-01,  3.7162e-01, -3.1690e-01,  2.9151e-01,\n",
      "          5.8462e-02, -6.1372e-01, -2.3874e-01,  3.6812e-01, -3.4587e-02,\n",
      "          7.0075e-01, -2.9594e-01,  3.2229e-01,  2.8400e-03,  2.3719e-01,\n",
      "          8.1043e-01,  6.9595e-01,  3.4920e-01, -3.7094e-01, -4.4487e-02]])\n"
     ]
    }
   ],
   "source": [
    "encoder.train()\n",
    "decoder.train()\n",
    "loss = 0\n",
    "\n",
    "# initialize encoder hidden\n",
    "hidden = encoder.initialize_hidden((len(en_sentence)))\n",
    "e_out, e_hidden = encoder(torch.from_numpy(inputs).long(), hidden)\n",
    "\n",
    "# print a random embedding vector before train model\n",
    "print(\"Before training encoder : \" + str(encoder.word_embeds(torch.LongTensor([21]))))\n",
    "print(\"Before training decoder : \" + str(decoder.word_embeds(torch.LongTensor([23]))))\n",
    "\n",
    "# clear old gradient from last step\n",
    "# need to clear after every optimizer step or it accumulate all gradients\n",
    "encoder_optimizer.zero_grad()\n",
    "decoder_optimizer.zero_grad()\n",
    "for i in range(len(en_sentence)):\n",
    "    \n",
    "    # initailize decoder hidden with encoder hidden for each sentence\n",
    "    decoder_hidden = (e_hidden[0][0][i].view(1,1,-1),e_hidden[1][0][i].view(1,1,-1))\n",
    "    \n",
    "    # initial input for decoder\n",
    "    decoder_input = torch.LongTensor([bn_vocab.token_To_index['_sos']])\n",
    "    output = []\n",
    "    for j in range(length):\n",
    "        decoder_output, decoder_hidden = decoder.forward(decoder_input, decoder_hidden)\n",
    "        \n",
    "        # choose the top value and it's index\n",
    "        top_value, top_index = decoder_output.topk(1)\n",
    "        output.append(decoder_output.view(1,-1))\n",
    "        \n",
    "        # initialize the next decoder input\n",
    "        decoder_input = torch.tensor([top_index.item()])\n",
    "    \n",
    "    # calculate loss of a sentence\n",
    "    loss += F.nll_loss(torch.cat(output), torch.from_numpy(targets[i]).long())\n",
    "loss = loss / len(en_sentence)\n",
    "\n",
    "# backward gradient\n",
    "loss.backward()\n",
    "\n",
    "# update parameters\n",
    "encoder_optimizer.step()\n",
    "decoder_optimizer.step()\n",
    "\n",
    "# print same embedding vector after train model\n",
    "print(\"After training encoder : \" + str(encoder.word_embeds(torch.LongTensor([21]))))\n",
    "print(\"After training decoder : \" + str(decoder.word_embeds(torch.LongTensor([23]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
