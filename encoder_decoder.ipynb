{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipynb.fs.full.batch import *\n",
    "from ipynb.fs.full.vocab import *\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LstmEncoder(nn.Module):\n",
    "    def __init__(self, in_vocab_size, word_dim, num_layer = 1, bidirection = False, pad_idx = None):\n",
    "        # in_vocab_size number of word in input language\n",
    "        # word_dim convert a word index into it's feature dimention (word_dim)\n",
    "        # pad_idx default None if provide then calculate gradient zero for specific index\n",
    "        # for more clarification (pad_idx) check out nn.Embedding pytorch documentation \n",
    "        super(LstmEncoder, self).__init__()\n",
    "        \n",
    "        self.hidden_size = word_dim      # hidden size of the model\n",
    "        self.num_layer = num_layer       # number of rnn model layers in encoder\n",
    "                                         # more than one layers also known as stacked rnn model\n",
    "        self.bidirection = bidirection   # direction of rnn model layers\n",
    "        \n",
    "        # initialize LSTM model for encoder\n",
    "        self.lstm = nn.LSTM(input_size=word_dim, hidden_size=word_dim, num_layers=num_layer, batch_first=True, bidirectional=bidirection)\n",
    "        \n",
    "        # initialize embedding model with padding index\n",
    "        if pad_idx != None:\n",
    "            self.word_embeds = nn.Embedding(num_embeddings = in_vocab_size, embedding_dim = word_dim, padding_idx = pad_idx)\n",
    "        \n",
    "        # initialize embedding model without padding index\n",
    "        else:\n",
    "            self.word_embeds = nn.Embedding(num_embeddings = in_vocab_size, embedding_dim = word_dim)\n",
    "    \n",
    "    # initialize zero for initial state of LSTM hidden and cell state\n",
    "    def initialize_hidden(self, batch_size):\n",
    "        # return states for non bidirectional LSTM\n",
    "        if self.bidirection == False:\n",
    "            return (torch.zeros(self.num_layer, batch_size, self.hidden_size),\n",
    "                    torch.zeros(self.num_layer, batch_size, self.hidden_size)\n",
    "                   )\n",
    "        # return states for bidirectional LSTM\n",
    "        else:\n",
    "            return (torch.zeros((self.num_layer*self.bidirection), batch_size, self.hidden_size),\n",
    "                    torch.zeros((self.num_layer*self.bidirection), batch_size, self.hidden_size)\n",
    "                   )\n",
    "    # Forward pass through LSTM\n",
    "    def forward(self, inputs, hidden):\n",
    "        # get the feature vector for corresponding word index\n",
    "        embed = self.word_embeds(inputs)\n",
    "        \n",
    "        # pass feature vector and tuple of (hidden, cell) states\n",
    "        output, hidden = self.lstm(embed, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "class LstmDecoder(nn.Module):\n",
    "    def __init__(self, tar_vocab_size, word_dim, num_layer = 1, pad_idx = None):\n",
    "        # tar_vocab_size number of word in target language\n",
    "        # word_dim convert a word index into it's feature dimention (word_dim)\n",
    "        # pad_idx default None if provide then calculate gradient zero for specific index\n",
    "        # for more clarification (pad_idx) check out nn.Embedding pytorch documentation \n",
    "        super(LstmDecoder, self).__init__()\n",
    "        \n",
    "        self.hidden_size = word_dim     # hidden size of the model\n",
    "        self.num_layer = num_layer      # number of rnn model layers in decoder\n",
    "                                        # more than one layers also known as stacked rnn model\n",
    "        \n",
    "        # initialize LSTM model for decoder\n",
    "        self.lstm = nn.LSTM(input_size = word_dim, hidden_size = word_dim, num_layers = num_layer)\n",
    "        \n",
    "        # a linear transformation for convert decoder output into tar_vocab_size\n",
    "        self.classifier = nn.Linear(self.hidden_size, tar_vocab_size)\n",
    "        \n",
    "        # initialize embedding model with padding index\n",
    "        # if padding index provide then gradient will be zero for that specific index\n",
    "        if pad_idx != None:\n",
    "            self.word_embeds = nn.Embedding(num_embeddings = tar_vocab_size, embedding_dim = word_dim, padding_idx = pad_idx)\n",
    "        \n",
    "        # initialize embedding model without padding index\n",
    "        else:\n",
    "            self.word_embeds = nn.Embedding(num_embeddings = tar_vocab_size, embedding_dim = word_dim)\n",
    "    def forward(self, inputs, hidden):\n",
    "        # get the feature vector for corresponding word index\n",
    "        embed = self.word_embeds(inputs).view(1,1,-1)\n",
    "        \n",
    "        # pass feature vector and tuple of (hidden, cell) states\n",
    "        outputs, hidden = self.lstm(embed, hidden)\n",
    "        \n",
    "        # linearly transform the output into it's vocab size\n",
    "        outputs = F.log_softmax(self.classifier(outputs.squeeze(1)), dim=1)\n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
